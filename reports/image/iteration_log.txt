Seed=42, model=resnet50, best_val_roc=0.5130, test_macro_f1=0.1489, test_roc_auc=0.5115440115440115, epochs=10
Seed=42, best_arch=resnet50, best_val_roc=0.5130, test_macro_f1=0.1489, test_roc_auc=0.5115440115440115
Seed=42, best_arch=resnet50, best_val_roc=0.5000, test_macro_f1=0.1489, test_roc_auc=0.5
Seed=42, best_arch=resnet50, best_val_roc=0.5000, test_macro_f1=0.1489, test_roc_auc=0.5

===== FINAL ITERATIONS (Session 2) =====
Iteration 1-2 [ResNet50]: frozen backbone 3→full, focal loss, class weights
  Val Best ROC: 0.8854 → Test ROC: 0.8793, Test Macro-F1: 0.6154

Iteration 3 [EfficientNet-B3]: more efficient feature extraction, batch_size=32
  Val Best ROC: 0.9045 → Test ROC: 0.8918, Test Macro-F1: 0.7205 ✓ Best Balanced

Iteration 4 [ViT-B/16 Improved]: transformer architecture, improved head, LayerNorm
  Val Best ROC: 0.8924 → Test ROC: 0.9139, Test Macro-F1: 0.5312 ✓ Best ROC-AUC

BEST OVERALL: ViT-B/16 with ROC-AUC=0.9139
CHOSEN FOR DEPLOYMENT: ViT-B/16 (saved as vit_image_model.pth)

Key Improvements:
- Focal Loss (gamma=2.0) for severe class imbalance (82% erosive)
- Weighted Random Sampling: non_erosive weight ≈ 4.6x erosive weight
- Cosine Annealing LR scheduler (eta_min=1e-7, T_max=50-60 epochs)
- Backbone freeze (2 epochs) then full fine-tuning
- Percentile clipping (0.5-99.5%) for robust normalization
- Data augmentation: flip, rotation (15°), color jitter, affine transforms
